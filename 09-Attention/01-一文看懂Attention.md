# 一文看懂 Attention

[一文看懂Attention](https://easyaitech.medium.com/%E4%B8%80%E6%96%87%E7%9C%8B%E6%87%82-attention-%E6%9C%AC%E8%B4%A8%E5%8E%9F%E7%90%86-3%E5%A4%A7%E4%BC%98%E7%82%B9-5%E5%A4%A7%E7%B1%BB%E5%9E%8B-e4fbe4b6d030)

## Attention 的本质是什么

1. Attention (注意力)机制如果浅层的理解, 跟它的名字非常匹配. 它的核心逻辑就是「从关注全部到关注重点」.

2. Attention 机制很像人类看图片的逻辑, 当我们看一张图片的时候我们并没有看清图片的全部内容, 而是将注意力集中在了图片的焦点上.

## Attention 的3大优点
1. 参数少
- 模型复杂度跟 CNN, RNN 相比, 复杂度更小, 参数也更少. 所以对算力的要求也就更小.

2. 速度快
- Attention 解决了 RNN 不能并行计算的问题. Attention机制每一步计算不依赖于上一步的计算结果, 因此可以和CNN一样并行处理

3. 效果好
- 在 Attention 机制引入之前, 有一个问题大家一直很苦恼: 长距离的信息会被弱化, 就好像记忆能力弱的人, 记不住过去的事情是一样的.
- Attention 是挑重点, 就算文本比较长, 也能从中间抓住重点, 不丢失重要的信息.