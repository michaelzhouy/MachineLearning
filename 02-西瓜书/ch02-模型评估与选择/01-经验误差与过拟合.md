# 经验误差与过拟合

1.  错误率：分类错误的样本数占样本总数的比例，$m$个样本中有$a$个样本分类错误，即错误率$E=a/m$，精度$acc=1-a/m$

2.  误差：学习器的实际预测输出与样本的真实输出之间的差异称为“误差”。学习器在训练集上的误差称为“训练误差”或“经验误差”，在新样本上的误差称为“泛化误差”

3.  过拟合、欠拟合

    ![](https://i.loli.net/2018/10/17/5bc7181172996.png)

---
# 评估方法

1.  我们希望使用一个测试集的测试误差作为泛化误差的近似，因而需要对数据进行有效的划分，划分出互斥的训练集和测试集

## 留出法

1.  将数据集$D$划分出两个互斥的集合$S$（训练集）和$T$（测试集），即$D=S\bigcup D, S\bigcap T=\emptyset$，在$S$上训练出模型后，用$T$来评估其测试误差，作为对泛化误差的估计
2.  训练/测试集的划分要尽可能保持数据分布的一致性，采用分层抽样
3.  由于划分的随机性，单次使用留出法得到的估计结果也往往不够稳定，一般采用多次随机划分取平均的做法
4.  一般，将大约==2/3~4/5==的样本用于训练，剩余样本用于测试

## 交叉验证法

1.  “交叉验证法”，首先将数据集$D$划分成$k$个大小相似的互斥子集，即$D=D_1\bigcup D_2\bigcup ...\bigcup D_k, D_i\bigcap D_j=\emptyset(i\neq j)$
2.  每个子集$D_i$尽可能保持数据分布的一致性，即从$D$中分层采样得到
3.  每次用$k-1$个子集的并集作为训练集，余下的那个子集作为测试集，这样就可获得$k$组训练/测试集，并行$k$次训练和测试，最终返回的是这$k$个测试结果的均值
4.  $p$次$k$交叉验证

## 自助法

1.  我们希望是用$D$训练出来的模型。但在留出法和交叉验证法中，由于保留了一部分样本用于测试，因此实际评估的模型所使用的训练集比$D$小，就是引入一些因训练样本规模不同而导致的估计偏差。留一法受训练样本规模变化的影响较小，但是计算复杂度由太高，“自助法”是一个比较好的解决方案

2.  自助法：对于样本大小为$m$的数据集$D$，做有放回采样，重复m次，得到包含m个样本的数据集$D'$。

3.  通过自采样，初始数据集$D$中大约有$36.8%$的样本没有出现在$D'$中，于是将$D'$作为训练集，$D\D'$作为测试集

4.  自助法产生的数据集改变了初始数据集的分布，这会引入估计偏差

## 调参与最终模型
1. 算法的参数，又称超参数
2. 模型的参数
3. 将训练集划分为训练集和验证集，用验证集来进行模型选择和调参，最后用整个训练集重新训练模型，并提交

---

# 性能度量

1.  性能度量：衡量模型泛化能力的评价标准

## 回归任务

1.  均方误差

$$E(f;D)=\frac{1}{m}\sum_{i=1}^{m}(f(x_i)-y_i)^2$$

## 分类任务

1.  错误率
$$
E(f;D)=\frac{1}{m}\sum_{i=1}^{m}Ⅱ(f(x_i)\neq y_i)
$$
2.  精度
$$
acc(f;D)=\frac{1}{m}\sum_{i=1}^mⅡ(f(x_i)=y_i)=1-E(f;D)
$$
3. 查准率P、查全率R、$F1$

-   查准率P(命中率)：预测为坏且真实为坏的数目/预测为坏的总数，取值越大说明预测的越准确
-   查全率R(召回率)：预测为坏且真实为坏的数目/真实为坏的总数，取值越大说明找到了更多的坏客户
-   $F1$：$F1=\frac{2\times P \times R}{P+R}$
-   $PR曲线$

4.  ROC与AUC

-   学习器对每个测试样本预测出一个概率，设定一个阈值，大于该阈值的分为正类，反之为反类。



# 偏差与方差

1.  方差：预测输出与预测输出的期望之间的差异，刻画的数据扰动所造成的影响
2.  偏差：预测输出与真实输出之间的差异，刻画的是学习算法本身的拟合能力